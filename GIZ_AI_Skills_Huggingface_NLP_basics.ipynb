{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibra56/Linear-regression-model/blob/main/GIZ_AI_Skills_Huggingface_NLP_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introductory Workshop on NLP with Transformers and Hugging Face\n",
        "\n",
        "Welcome to our introductory workshop on Natural Language Processing (NLP) using Transformers and the Hugging Face library. This session is designed to demystify NLP concepts and provide you with hands-on experience in sentiment analysis and topic classification.\n",
        "\n",
        "## Introduction to NLP and Transformers\n",
        "\n",
        "### What is NLP?\n",
        "\n",
        "Natural Language Processing (NLP) is an exciting field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal of NLP is to enable computers to understand, interpret, and respond to human languages in a valuable way. Applications of NLP include voice-activated GPS systems, digital assistants, automatic translation services, and many more.\n",
        "\n",
        "### The Revolution of Transformers\n",
        "\n",
        "Transformers have revolutionized the way machines understand human language. Unlike previous models that processed words in sequence, transformers process all words in a sentence simultaneously. This parallel processing allows the model to understand the context of each word more effectively. Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are examples of how transformers have set new standards for NLP tasks.\n",
        "\n",
        "Older NLP architectures mostly  relied on vanilla RNNs(shown below) and LSTMs.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc\" width=\"60%\"/>\n",
        "</div>\n",
        "\n",
        "These have all but been replaced by transformers\n",
        "\n",
        "<!-- ![transformer](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png) -->\n",
        "\n",
        "<div>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"60%\"/>\n",
        "</div>\n",
        "\n",
        "## Getting Started with Hugging Face Transformer Library\n",
        "\n",
        "Before we dive into the practical exercises, let's set up our environment by installing the Hugging Face `transformers` library in Google Colab.\n",
        "\n",
        "```python\n",
        "!pip install transformers\n",
        "```"
      ],
      "metadata": {
        "id": "Uz_VEJoR3IvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face provides a vast ecosystem for NLP, including **pre-trained models, datasets, and tools** for efficient machine learning workflows.\n",
        "\n"
      ],
      "metadata": {
        "id": "V9FcRkUi3eoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyEuWJToyG_x",
        "outputId": "0716d93b-03b1-4bbf-eded-fc31e90753b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Sentiment Analysis\n",
        "## Theoretical Background\n",
        "Sentiment analysis is a technique used in NLP to determine the emotional tone behind words. It's widely used in analyzing opinions from reviews, social media, and more.\n",
        "\n",
        "## Practical Coding Exercise\n",
        "Implement sentiment analysis using a pre-trained model from Hugging Face:"
      ],
      "metadata": {
        "id": "hBD2TPl90xCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze the sentiment of a sentence\n",
        "result = sentiment_pipeline(\"I don't like  transformers for NLP tasks!\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOKAlTzByR01",
        "outputId": "6c31ea0a-6df2-4615-cfad-ac028d1dc861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9981619715690613}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads a pre-trained sentiment analysis model and processes a sample sentence to determine its sentiment.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zwQSVoah0_w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2roZpXMU1KbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "lNboZc4pzdm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1** : Your task is to write a Python function that processes a list of sentences and returns their sentiment scores.\n",
        "\n",
        "Your api should be as follows:\n",
        "\n",
        "```python\n",
        "sentences = [\"I love learning about AI!\", \"This workshop is challenging but rewarding.\", \"I'm not sure I understand all of this.\"]\n",
        "print(analyze_sentiments(sentences))\n",
        "```\n",
        "\n",
        "**Exercise 2:** Try 3 different models and draw a table comparing their performance e.g:\n",
        "\n",
        "```python\n",
        "specific_model = pipeline(model=\"finiteautomata/ bertweet-base-sentiment-analysis\")\n",
        "specific_model(data)\n",
        "```\n",
        "Select from over 3,000 models on HF: https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment\n",
        "\n",
        "**Exercise 3:** Create your own sentiment analysis dataset using Uganda-centric data e.g. scrape a news site, governement site, social media, dataverse etc. Your dataset should contain atleast 100 sentences.\n",
        "\n",
        "**Exercise  4**: Create an account on hugging face and upload your best model and datasets to huggingface and submit the inference link.\n",
        "\n",
        "See:  https://huggingface.co/docs/datasets/v1.16.0/upload_dataset.html and https://huggingface.co/transformers/v4.10.1/model_sharing.html"
      ],
      "metadata": {
        "id": "1da5mBclzMii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details including how to finetune yours: https://huggingface.co/blog/sentiment-analysis-python"
      ],
      "metadata": {
        "id": "MZf3y-eWy_8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Topic Classification\n",
        "## Theoretical Background\n",
        "Topic classification is about identifying the main themes or topics in a piece of text. It's essential for content categorization, information retrieval, and personalized content recommendations.\n",
        "\n",
        "## Practical Coding Exercise\n",
        "Next, we'll perform topic classification using another pre-trained model."
      ],
      "metadata": {
        "id": "xT-9vlD-2IXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classification pipeline\n",
        "topic_classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Classify the topic of a given text\n",
        "result = topic_classifier(\"Transformers are revolutionizing the field of NLP.\", candidate_labels=[\"technology\", \"health\", \"finance\", \"education\", \"agriculture\"])\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DZW0p_i2Qqa",
        "outputId": "089f76d0-8d1e-4fb4-f8cc-d2185dd3e409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'Transformers are revolutionizing the field of NLP.', 'labels': ['technology', 'education', 'health', 'finance', 'agriculture'], 'scores': [0.9755109548568726, 0.008430569432675838, 0.0084175830706954, 0.003975448198616505, 0.003665406256914139]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates how to classify a text into predefined categories, even if the model has not been explicitly trained on those categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "nTUcMNyn26ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises:\n",
        "**Exercise 1**: Develop a function that classifies a list of texts into categories and counts the occurrences of each category.\n",
        "\n",
        "**Exercise 2**: Collect or find a dataset with text in atleast 4 categories and test your pipeline in Exercise 1.\n",
        "\n",
        "**Exercise  3**: Upload your best model and datasets to huggingface and submit the inference link."
      ],
      "metadata": {
        "id": "FIaO-yI02bpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to experiment with different texts and categories. Reflect on how sentiment analysis and topic classification can be applied in various domains.\n",
        "\n"
      ],
      "metadata": {
        "id": "ufNO8Re22v_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outro"
      ],
      "metadata": {
        "id": "muG6MZ3c9jkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many other NLP tasks that can be solved with transformers. They also happen to be the foundational architecture for all modern LLMs that has birthed general intelligence beyond which humans thought was possible in machines before."
      ],
      "metadata": {
        "id": "Sdtqi1yd8UhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key References\n",
        "\n",
        "1. **Hugging Face's Transformers Library Documentation:** Provides comprehensive guides and tutorials.  \n",
        "   [Hugging Face Documentation](https://huggingface.co/docs/transformers/index)\n",
        "\n",
        "2. **\"Attention is All You Need\":** The seminal paper introducing transformers.  \n",
        "   [Attention Paper](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "3. **NLP Course by Hugging Face:** A free course on NLP that covers the basics to advanced topics.  \n",
        "   [Hugging Face Course](https://huggingface.co/course/chapter1)\n",
        "\n",
        "4. **YouTube Videos for Visual Learners:**  \n",
        "   - [Introduction to NLP](https://www.youtube.com/watch?v=fOvTtapxa9c) by Stanford University.  \n",
        "   - [Transformers and Hugging Face](https://www.youtube.com/watch?v=KGPjcdOiXtg) for a hands-on tutorial on using transformers.\n",
        "5. **CS30 Stanford:**\n",
        "  - https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks"
      ],
      "metadata": {
        "id": "M6rvHH3M8YOY"
      }
    }
  ]
}